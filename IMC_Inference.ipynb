{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用LoFTR模型直接推理，Public LB 0.726；\n",
    "\n",
    "对LoFTR进行调参（分辨率/keypoints数量/threshold等），Public LB 0.767；\n",
    "\n",
    "使用SuperGlue并调参，Public LB : 0.710；\n",
    "\n",
    "使用QTA并调参，Public LB : 0.799；\n",
    "\n",
    "使用DKM并调参，Public LB : 0.600；\n",
    "\n",
    "对四个模型结果进行融合，Public LB : 0.830+；\n",
    "\n",
    "对cv2.findFundamentalMat进行调参，Public LB : 0.840+；\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装/导入 model lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "!pip install ../input/pywheels/loguru-0.5.3-py3-none-any.whl\n",
    "!pip install ../input/pywheels/einops-0.4.1-py3-none-any.whl\n",
    "!pip install ../input/pywheels/timm-0.4.12-py3-none-any.whl\n",
    "\n",
    "#kornia\n",
    "!pip install /kaggle/input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n",
    "!pip install /kaggle/input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n",
    "!pip install ../input/pywheels/pydegensac-0.1.2-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "# install Quadree attention\n",
    "!cp -r ../input/othermodels/github_QuadTreeAttention-master_renamed_pkg_src2 /kaggle/working/ # input folder is read only\n",
    "!cd /kaggle/working/github_QuadTreeAttention-master_renamed_pkg_src2/QuadTreeAttention/ && pip install .\n",
    "sys.path.append(\"/kaggle/working/github_QuadTreeAttention-master_renamed_pkg_src2\")\n",
    "sys.path.append(\"/kaggle/working/github_QuadTreeAttention-master_renamed_pkg_src2/FeatureMatching/\")\n",
    "sys.path.append(\"/kaggle/working/github_QuadTreeAttention-master_renamed_pkg_src2/QuadTreeAttention/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "import PIL\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kornia\n",
    "from kornia_moons.feature import *\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "import random\n",
    "import string\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "src = '/kaggle/input/image-matching-challenge-2022/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 QTA\n",
    "from FeatureMatching.src2.config.default import get_cfg_defaults as get_cfg_defaults_qta\n",
    "from FeatureMatching.src2.utils.profiler import build_profiler as build_profiler_qta\n",
    "from FeatureMatching.src2.lightning.lightning_loftr import PL_LoFTR as PL_LoFTR_qta\n",
    "\n",
    "config = get_cfg_defaults_qta() # QTA config\n",
    "CROP_RATIO = 3 # QTA crop ratio\n",
    "TRANSFORMS = None # QTA transforms\n",
    "NB_EPOCHS = 5 # QTA epochs\n",
    "\n",
    "# INDOOT lofrt_ds_quadtree config\n",
    "config.LOFTR.MATCH_COARSE.MATCH_TYPE = 'dual_softmax' # QTA match type\n",
    "config.LOFTR.MATCH_COARSE.SPARSE_SPVS = False # QTA sparse spvs\n",
    "config.LOFTR.RESNETFPN.INITIAL_DIM = 128 # QTA initial dim\n",
    "config.LOFTR.RESNETFPN.BLOCK_DIMS=[128, 196, 256] # QTA block dims\n",
    "config.LOFTR.COARSE.D_MODEL = 256 # QTA model dim     transformer里面的维度大小，这个是需要去调整的\n",
    "config.LOFTR.COARSE.BLOCK_TYPE = 'quadtree' # QTA block type\n",
    "config.LOFTR.COARSE.ATTN_TYPE = 'B' # QTA attention type\n",
    "config.LOFTR.COARSE.TOPKS=[32, 16, 16] # QTA topks\n",
    "config.LOFTR.FINE.D_MODEL = 128 # QTA model dim\n",
    "config.TRAINER.WORLD_SIZE = 1 # QTA world size\n",
    "config.TRAINER.CANONICAL_BS = 32 # QTA canonical batch size\n",
    "config.TRAINER.TRUE_BATCH_SIZE = 1 # QTA true batch size        学习率是可以调的\n",
    "_scaling = 1 # QTA scaling\n",
    "config.TRAINER.ENABLE_PLOTTING = False # QTA plotting\n",
    "config.TRAINER.SCALING = _scaling # QTA scaling\n",
    "config.TRAINER.TRUE_LR = 1e-3 # 1e-4 config.TRAINER.CANONICAL_LR * _scaling\n",
    "config.TRAINER.WARMUP_STEP = 0 #math.floor(config.TRAINER.WARMUP_STEP / _scaling)\n",
    "\n",
    "# lightning module\n",
    "qta_max_img_size = 1024 # max image size for QTA \n",
    "\n",
    "qta_device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # set device\n",
    "disable_ckpt = True # disable checkpoint\n",
    "profiler_name = None # help='options: [inference, pytorch], or leave it unset\n",
    "qta_profiler = build_profiler_qta(profiler_name) # QTA profiler\n",
    "qta_model = PL_LoFTR_qta(config, # QTA model\n",
    "                 pretrained_ckpt= \"/kaggle/working/outdoor_quadtree.ckpt\", # args.ckpt_path, from scratch atm\n",
    "                 profiler=qta_profiler # QTA profiler\n",
    "                 )\n",
    "qta_matcher = qta_model.matcher # get matcher\n",
    "qta_matcher.eval() # set eval mode\n",
    "qta_matcher.to(qta_device) # move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image utils\n",
    "def load_resized_image(fname, max_image_size):\n",
    "    '''\n",
    "    Load image and resize it to max_image_size 最长边 扩大至指定尺寸\n",
    "    '''\n",
    "    img = cv2.imread(fname) # load image\n",
    "    scale = max_image_size / max(img.shape[0], img.shape[1])  # get scale\n",
    "    w = int(img.shape[1] * scale) # weight\n",
    "    h = int(img.shape[0] * scale) # height\n",
    "    img = cv2.resize(img, (w, h)) \n",
    "    return img, scale\n",
    "\n",
    "def scale_to_resized(mkpts0, mkpts1, scale1, scale2):\n",
    "    '''\n",
    "    scale to original image size because we used max_image_size\n",
    "    '''\n",
    "    # first point\n",
    "    mkpts0[:, 0] = mkpts0[:, 0] / scale1\n",
    "    mkpts0[:, 1] = mkpts0[:, 1] / scale1\n",
    "    \n",
    "    # second point\n",
    "    mkpts1[:, 0] = mkpts1[:, 0] / scale2\n",
    "    mkpts1[:, 1] = mkpts1[:, 1] / scale2\n",
    "    \n",
    "    return mkpts0, mkpts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QTA Utils\n",
    "def load_loftr_image_orig(fname):\n",
    "    img0_raw = cv2.imread(fname, cv2.IMREAD_GRAYSCALE) # 载入图片\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255. # 255归一化\n",
    "    return img0\n",
    "\n",
    "def put_img_on_disk(img, output_img_tag):\n",
    "    img_path_on_disk = f'/kaggle/working/{output_img_tag}.png'\n",
    "    cv2.imwrite(img_path_on_disk, img) # save image\n",
    "    return img_path_on_disk\n",
    "\n",
    "def calc_divide_size_smallest(im_size, coef):\n",
    "    '''\n",
    "    image size 必须是 coef 的倍数\n",
    "    '''\n",
    "    # select size dividable by coef\n",
    "    if im_size % coef == 0:\n",
    "        return im_size   # 能够整除，就返回原值\n",
    "    return round(((im_size / coef) + 0.5)) * coef\n",
    "\n",
    "\n",
    "def add_zero_padding_two_img_same(img1, img2, div_coef=32):\n",
    "    '''\n",
    "    add zero padding to two images to make them same size\n",
    "    '''\n",
    "\n",
    "    img1_height, img1_width, img1_channels = img1.shape # 获取图片的高度、宽度、通道数\n",
    "    img2_height, img2_width, img2_channels = img2.shape # 获取图片的高度、宽度、通道数\n",
    "    \n",
    "    # fit both images on canvas\n",
    "    max_width = max(img1_width, img2_width) # 最大宽度\n",
    "    max_height = max(img1_height, img2_height) # 最大高度\n",
    "    \n",
    "    # use own width and height for image with zero-padding\n",
    "    result1, offset1 = create_zero_padding_img(img1, max_width, max_height, img1_channels, div_coef) # 填充图片1\n",
    "    result2, offset2 = create_zero_padding_img(img2, max_width, max_height, img2_channels, div_coef) # 填充图片2\n",
    "    \n",
    "    return result1, result2, offset1, offset2\n",
    "\n",
    "\n",
    "def create_zero_padding_img(img, max_im_width, max_im_height, channels, div_coef=32):\n",
    "    # create new image of desired size and color (black) for padding\n",
    "    new_area_image_width = calc_divide_size_smallest(max_im_width, div_coef) # 计算新的宽度\n",
    "    new_area_image_height = calc_divide_size_smallest(max_im_height, div_coef) # 计算新的高度\n",
    "    \n",
    "    # it is different from what we have in max_im_width and max_im_height\n",
    "    # max_im_height, max_im_width define an area for image and maybe include extra mask\n",
    "    im_height, im_width, im_channels = img.shape # 获取图片的高度、宽度、通道数\n",
    "    x_offset = (new_area_image_width - im_width) // 2 # 计算x轴偏移量\n",
    "    y_offset = (new_area_image_height - im_height) // 2 # 计算y轴偏移量\n",
    "    \n",
    "    im_right  = x_offset + im_width  # right of image corner where ends \n",
    "    im_bottom = y_offset + im_height # right of image corner where ends\n",
    "    \n",
    "    \n",
    "    color = (0,0,0) # 置零\n",
    "    result = np.full((new_area_image_height, new_area_image_width, im_channels), color, dtype=np.uint8) # 创建全0图片\n",
    "    \n",
    "    # copy img image into center of result image\n",
    "    result[y_offset:im_bottom, x_offset:im_right] = img # 将图片拷贝到中心位置\n",
    "    \n",
    "    # return image and x,y of old image in a new image (frame)\n",
    "    return result, (x_offset, y_offset) # 返回新的图片 和 原图片的偏移量\n",
    "\n",
    "\n",
    "def unpad_matches(mkpts0, mkpts1, offset_point1, offset_point2):\n",
    "    '''\n",
    "    remove padding from two images\n",
    "    '''\n",
    "    offset_x1, offset_y1 = offset_point1 # 获取图片1偏移量\n",
    "    offset_x2, offset_y2 = offset_point2 # 获取图片2偏移量\n",
    "    # remove points on padding\n",
    "     # 去除偏移量\n",
    "    mkpts0[:, 0] = mkpts0[:, 0] - offset_x1\n",
    "    mkpts0[:, 1] = mkpts0[:, 1] - offset_y1\n",
    "    \n",
    "    mkpts1[:, 0] = mkpts1[:, 0] - offset_x2\n",
    "    mkpts1[:, 1] = mkpts1[:, 1] - offset_y2\n",
    "    return mkpts0, mkpts1\n",
    "\n",
    "\n",
    "\n",
    "# QTA inference\n",
    "def qta_inference(image_fpath_1, image_fpath_2, max_image_size=qta_max_img_size, divide_coef=32):\n",
    "    # https://github.com/zju3dv/LoFTR/blob/master/notebooks/demo_single_pair.ipynb\n",
    "    # resize image if we need\n",
    "    img1_resized, scale1 = load_resized_image(image_fpath_1, max_image_size) # image1 最长边 扩大至指定尺寸\n",
    "    img2_resized, scale2 = load_resized_image(image_fpath_2, max_image_size) # image2 最长边 扩大至指定尺寸\n",
    "    \n",
    "    ### add padding -> use same image padding mask because models wants it\n",
    "    pad_img1, pad_img2, pad_offset_p1, pad_offset_p2 = add_zero_padding_two_img_same(img1_resized, img2_resized, divide_coef) # padding图片\n",
    "\n",
    "    # save temporarily 保存新图片到磁盘    \n",
    "    img1_disk_path = put_img_on_disk(pad_img1, 'qta_img1')\n",
    "    img2_disk_path = put_img_on_disk(pad_img2, 'qta_img2')\n",
    "    \n",
    "    # 读取新图片，并做255归一化\n",
    "    gray_img_1 = load_loftr_image_orig(img1_disk_path) \n",
    "    gray_img_2 = load_loftr_image_orig(img2_disk_path)\n",
    "    \n",
    "    batch = {'image0': gray_img_1, 'image1': gray_img_2}\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        qta_matcher.eval() # 取消梯度计算\n",
    "        qta_matcher.to(qta_device) # 将模型放到GPU\n",
    "        \n",
    "        qta_matcher(batch) # 运行模型\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy() # mkpts0\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy() # mkpts1\n",
    "    \n",
    "    ### unpad matches\n",
    "    mkpts0, mkpts1 = unpad_matches(mkpts0, mkpts1, pad_offset_p1, pad_offset_p2) # 去除padding\n",
    "    \n",
    "    ### scale to original im size because we used max_image_size\n",
    "    mkpts0, mkpts1 = scale_to_resized(mkpts0, mkpts1, scale1, scale2) # 将图像缩放到原图大小\n",
    "    \n",
    "    # clean up 删除临时文件\n",
    "    if os.path.exists(img1_disk_path): os.remove(img1_disk_path)\n",
    "    if os.path.exists(img2_disk_path): os.remove(img2_disk_path) \n",
    "    \n",
    "    return mkpts0, mkpts1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoFTR + SuperGlue + DKM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoFTR\n",
    "sys.path.append('../input/othermodels/github_LoFTR-master') \n",
    "\n",
    "# Kornia, a dual-softmax operator\n",
    "# models https://drive.google.com/drive/folders/1xu2Pq6mZT5hmFgiYMBT9Zt8h1yO-3SIp\n",
    "\n",
    "kornia_max_image_size = 1120 # max image size for LoFTR 表示 LoFTR 算法处理的图像的最大尺寸是 1120 像素；\n",
    "kornia_at_least_matches = 280 # at least matches for LoFTR 表示在 LoFTR 算法中至少需要匹配 280 个特征点，才能被认为是有效的匹配；\n",
    "kornia_thrs_conf_match = 0.3 # threshold for confidence match for LoFTR 表示在 LoFTR 算法中特征点匹配的置信度阈值为 0.3；\n",
    "kornia_max_matches = 1000 # -1 -> use all 表示在 LoFTR 算法中最多匹配 1000 个特征点。如果设置为 -1，则使用所有的特征点进行匹配。\n",
    "\n",
    "kf_loftr_out_device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda') # use cpu or cuda\n",
    "kf_loftr_out_matcher = KF.LoFTR(pretrained=None) # load model\n",
    "kf_loftr_out_matcher.load_state_dict(torch.load(\"/kaggle/input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict']) # load weights\n",
    "kf_loftr_out_matcher = kf_loftr_out_matcher.to(kf_loftr_out_device).eval() # set to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SuperGlue\n",
    "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching\n",
    "from models.utils import (AverageTimer, read_image)\n",
    "\n",
    "sg_device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # use cpu or cuda\n",
    "# -1 before. -1 use original size, 1600 -> recommended size\n",
    "resize = [1600, ] # resize to this size\n",
    "resize_float = True # use float32\n",
    "\n",
    "# new isamu SG tuning parameters\n",
    "config = {\n",
    "    \"superpoint\": {\n",
    "        \"nms_radius\": 3,\n",
    "        \"keypoint_threshold\": 0.001, \n",
    "        \"max_keypoints\": 1280 \n",
    "    },\n",
    "    \"superglue\": {\n",
    "        \"weights\": \"outdoor\", \n",
    "        \"sinkhorn_iterations\": 20, \n",
    "        \"match_threshold\": 0.1,\n",
    "    }\n",
    "}\n",
    "sg_matching = Matching(config).eval().to(sg_device) # load model\n",
    "\n",
    "# filtering\n",
    "use_sg_filter_strategy = False # use SG filter strategy\n",
    "sg_max_confid_matches = 300 # max matches for SG\n",
    "sg_thrs_conf_match = 0.3 # threshold for confidence match for SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DKM\n",
    "!mkdir -p pretrained/checkpoints\n",
    "!cp ../input/othermodels/dkm_base_v11.pth pretrained/checkpoints/dkm_base_v11.pth\n",
    "\n",
    "!pip install -f ../input/pywheels --no-index einops \n",
    "!cp -r ../input/othermodels/github_dkm-main_fix_kaggle/ /kaggle/working/DKM/\n",
    "\n",
    "sys.path.append('/kaggle/working/DKM/')\n",
    "\n",
    "# DKM\n",
    "import torch\n",
    "torch.hub.set_dir('/kaggle/working/pretrained/')\n",
    "from dkm import dkm_base\n",
    "\n",
    " # load model\n",
    "dkm_model = dkm_base(\n",
    "                pretrained=True,\n",
    "                version=\"v11\",\n",
    "                use_cuda=True, \n",
    "                request_im_size=(672, 896) # resize to (h, w)\n",
    "                     )\n",
    "\n",
    "dkm_ft_num = 1000 # dkm match number\n",
    "dkm_max_confid_matches = 200 # max matches for confident\n",
    "dkm_thrs_conf_match = 0.3 # threshold for confidence match for DKM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device, max_image_size):\n",
    "    '''\n",
    "    Load image from file and convert to torch tensor. 最长边 扩大至指定尺寸\n",
    "    '''\n",
    "    img = cv2.imread(fname)\n",
    "    scale = max_image_size / max(img.shape[0], img.shape[1])  # 扩大倍数\n",
    "    w = int(img.shape[1] * scale)\n",
    "    h = int(img.shape[0] * scale)\n",
    "    img = cv2.resize(img, (w, h))\n",
    "    img = K.image_to_tensor(img, False).float() /255.\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img.to(device), scale\n",
    "\n",
    "\n",
    "def filter_conf_matches(mkpts0, mkpts1, mconf, thrs_conf_match=0.2, max_matches=-1):\n",
    "    '''\n",
    "    Filter matches by confidence.\n",
    "    '''\n",
    "    # sort points by confidence descending\n",
    "    mkps1_sorted = [x for (y,x) in sorted(zip(mconf,mkpts0), key=lambda pair: pair[0], reverse=True)] # 根据置信度降序排序image1的点\n",
    "    mkps2_sorted = [x for (y,x) in sorted(zip(mconf,mkpts1), key=lambda pair: pair[0], reverse=True)] # 根据置信度降序排序image2的点\n",
    "    \n",
    "    # overwrite\n",
    "    mkps1 = np.array(mkps1_sorted) \n",
    "    mkps2 = np.array(mkps2_sorted)\n",
    "\n",
    "    num_thrsh_greater = (mconf >= thrs_conf_match).sum() # 置信度大于阈值的点的数量\n",
    "\n",
    "    take_first_el = min(max_matches, num_thrsh_greater) # matches 数量限制\n",
    "    if take_first_el > 0 and len(mkps1) > take_first_el:\n",
    "        mkps1 = mkps1[:take_first_el] # image1 取前take_first_el个点\n",
    "        mkps2 = mkps2[:take_first_el] # image2 取前take_first_el个点\n",
    "    return mkps1, mkps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loftr_inference(fname1, fname2, max_image_size):    \n",
    "    '''\n",
    "    Infer matches using LoFTR.\n",
    "    '''\n",
    "    image_1, scale1 = load_torch_image(fname1, kf_loftr_out_device, max_image_size) # image1 最长边 扩大至指定尺寸\n",
    "    image_2, scale2 = load_torch_image(fname2, kf_loftr_out_device, max_image_size) # image2 最长边 扩大至指定尺寸\n",
    "    \n",
    "    # 颜色格式\n",
    "    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1),\n",
    "                  \"image1\": K.color.rgb_to_grayscale(image_2)\n",
    "              }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correspondences = kf_loftr_out_matcher(input_dict) # LoFTR模型 推理两张图片\n",
    "        \n",
    "    # mkpts0, mkpts1, confidence\n",
    "    mkpts0 = correspondences['keypoints0'].cpu().numpy() # keypoints0\n",
    "    mkpts1 = correspondences['keypoints1'].cpu().numpy() # keypoints1\n",
    "    \n",
    "    # 将坐标缩放到原图\n",
    "    mkpts0 = mkpts0 / scale1 \n",
    "    mkpts1 = mkpts1 / scale2     \n",
    "    \n",
    "    return mkpts0, mkpts1\n",
    "\n",
    "\n",
    "def superglue_inference(image_fpath_1, image_fpath_2):    \n",
    "    image_1, inp_1, scales_1 = read_image(image_fpath_1, sg_device, resize, 0, resize_float) # 读取image1，并resize\n",
    "    image_2, inp_2, scales_2 = read_image(image_fpath_2, sg_device, resize, 0, resize_float) # 读取image2，并resize\n",
    "    pred = sg_matching({\"image0\": inp_1, \"image1\": inp_2}) # superglue模型 推理两张图片\n",
    "    pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()} # 将tensor转为numpy\n",
    "    kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"] # kpts1 和 kpts2\n",
    "    matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"] # matches 和 conf\n",
    "\n",
    "    ####\n",
    "    valid = matches > -1 # 有效的点\n",
    "    mkpts1 = kpts1[valid]  # mkpts1\n",
    "    mkpts2 = kpts2[matches[valid]] # mkpts2\n",
    "    mconf = conf[valid] # mconf\n",
    "    \n",
    "    # scales_1 and scales_2 are equal because we resize to max size and keep aspect ratio \n",
    "    # scales_1 is a tuple (scale1, scale2)\n",
    "    scale1, scale2 = scales_1 # 扩大的倍数\n",
    "    \n",
    "    # because of https://github.com/magicleap/SuperGluePretrainedNetwork/blob/c0626d58c843ee0464b0fa1dd4de4059bfae0ab4/models/utils.py#L240\n",
    "    mkpts1, mkpts2 = scale_to_resized(mkpts1, mkpts2, 1./scale1, 1./scale2)\n",
    "        \n",
    "    # filter matches by confidence\n",
    "    if use_sg_filter_strategy:\n",
    "        mkpts1, mkpts2 = filter_conf_matches(   \n",
    "            mkpts1,\n",
    "            mkpts2,\n",
    "            mconf, \n",
    "            thrs_conf_match=sg_thrs_conf_match, # 0.3, 置信度阈值 \n",
    "            max_matches=sg_max_confid_matches,  # 300, matches 数量限制\n",
    "            )\n",
    "    \n",
    "    return mkpts1, mkpts2\n",
    "\n",
    "\n",
    "def dkm_inference(image_fpath_1, image_fpath_2):\n",
    "    # https://github.com/Parskatt/DKM\n",
    "    # internally it resizes to h=384 w=512    \n",
    "    img1 = cv2.imread(image_fpath_1) # 读取image1\n",
    "    img2 = cv2.imread(image_fpath_2) # 读取image2\n",
    "\n",
    "    # 将图片转为PIL格式\n",
    "    img1PIL = Image.fromarray(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)) \n",
    "    img2PIL = Image.fromarray(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)) \n",
    "    \n",
    "    #dense_matches, dense_certainty = dkm_model.match(img1PIL, img2PIL)\n",
    "    dense_matches, dense_certainty = dkm_model.match(img1PIL, img2PIL, use_cuda=True) # 模型推理, 使用cuda\n",
    "    \n",
    "    # # You may want to process these, e.g. we found dense_certainty = dense_certainty.sqrt() to work quite well in some cases.\n",
    "    dense_certainty = dense_certainty.sqrt() # 将置信度开根号\n",
    "    # Sample 10000 sparse matches\n",
    "    sparse_matches, sparse_certainty = dkm_model.sample(dense_matches, dense_certainty, dkm_ft_num) ####\n",
    "\n",
    "    mkps1 = sparse_matches[:, :2] # mkps1\n",
    "    mkps2 = sparse_matches[:, 2:] # mkps2\n",
    "\n",
    "    ####\n",
    "    h, w, c = img1.shape # \n",
    "    mkps1[:, 0] = ((mkps1[:, 0] + 1)/2) * w\n",
    "    mkps1[:, 1] = ((mkps1[:, 1] + 1)/2) * h\n",
    "\n",
    "    h, w, c = img2.shape\n",
    "    mkps2[:, 0] = ((mkps2[:, 0] + 1)/2) * w\n",
    "    mkps2[:, 1] = ((mkps2[:, 1] + 1)/2) * h\n",
    "    \n",
    "    \n",
    "    # filter matches by confidence\n",
    "    mkps1, mkps2 = filter_conf_matches(\n",
    "        mkps1,  # mkps1\n",
    "        mkps2,  # mkps2\n",
    "        sparse_certainty,  # mconf\n",
    "        thrs_conf_match=dkm_thrs_conf_match, # 0.3, 置信度阈值\n",
    "        max_matches=dkm_max_confid_matches,  # 200, matches 数量限制\n",
    "        )\n",
    "    \n",
    "    return mkps1, mkps2, sparse_certainty\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_models_ensemble(image_fpath_1, image_fpath_2):\n",
    "    '''\n",
    "    Input : image1 和 image2, 进行inference ensemble\n",
    "    Return: match keypoints (匹配关键点)\n",
    "    '''\n",
    "    # loftr\n",
    "    k_mkpts1, k_mkpts2 = loftr_inference(image_fpath_1, image_fpath_2, kornia_max_image_size) # kornia_max_image_size = 1120\n",
    "    mkpts1_merge = k_mkpts1\n",
    "    mkpts2_merge = k_mkpts2\n",
    "   \n",
    "    #  QuadTreeAttention\n",
    "    qta_mkps1,qta_mkps2 = qta_inference(image_fpath_1, image_fpath_2, max_image_size=qta_max_img_size, divide_coef=32) # qta_max_img_size = 1024\n",
    "    mkpts1_merge = np.concatenate((mkpts1_merge, qta_mkps1), axis=0)\n",
    "    mkpts2_merge = np.concatenate((mkpts2_merge, qta_mkps2), axis=0)\n",
    "    \n",
    "    # SUPERGLUE\n",
    "    sg_mkpts1, sg_mkpts2 = superglue_inference(image_fpath_1, image_fpath_2) \n",
    "    mkpts1_merge = np.concatenate((mkpts1_merge, sg_mkpts1), axis=0)\n",
    "    mkpts2_merge = np.concatenate((mkpts2_merge, sg_mkpts2), axis=0)\n",
    "\n",
    "    # DKM\n",
    "    dkm_mkpts1, dkm_mkpts2, _ = dkm_inference(image_fpath_1, image_fpath_2)\n",
    "    if len(dkm_mkpts1) > 0:\n",
    "        mkpts1_merge = np.concatenate((mkpts1_merge, dkm_mkpts1), axis=0)\n",
    "        mkpts2_merge = np.concatenate((mkpts2_merge, dkm_mkpts2), axis=0)\n",
    "\n",
    "    return mkpts1_merge, mkpts2_merge # get match keypoints\n",
    "\n",
    "\n",
    "def calc_F_matrix(item):\n",
    "    '''\n",
    "    Input : sample_id, mkps1, mkps2 # 一对匹配关键点\n",
    "    Return: F-matrix\n",
    "    '''\n",
    "    sample_id, mkps1, mkps2 = item \n",
    "\n",
    "    if len(mkps1) > 7: # 如果有足够的点, 则进行F-matrix计算\n",
    "        F, _ = cv2.findFundamentalMat(mkps1, mkps2, cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, confidence=0.999999, maxIters=120_000)  # 计算F-matrix\n",
    "    else:   # 如果没有足够的点, 则返回空的F-matrix\n",
    "        F = None   \n",
    "\n",
    "    del mkps1, mkps2 # clean up\n",
    "    return sample_id, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []\n",
    "with open(f'{src}/test.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        test_samples += [row]\n",
    "        \n",
    "test_samples_run_optim = test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入线程池\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "from concurrent import futures\n",
    "F_dict = {} # F-matrix\n",
    "prev_matches_item = None # previous matches item\n",
    "\n",
    "for i, (sample_id, batch_id, image_1_id, image_2_id) in enumerate(test_samples_run_optim):\n",
    "    image_fpath_1 = f'{src}/test_images/{batch_id}/{image_1_id}.png' # image1 path\n",
    "    image_fpath_2 = f'{src}/test_images/{batch_id}/{image_2_id}.png' # image2 path\n",
    "    \n",
    "    if prev_matches_item is None:\n",
    "        # 初始化 只获取匹配关键点\n",
    "        mkpts1_merge, mkpts2_merge = inf_models_ensemble(image_fpath_1, image_fpath_2) # get match keypoints\n",
    "        prev_matches_item = (sample_id, mkpts1_merge, mkpts2_merge) # update prev_matches_item\n",
    "    else:\n",
    "        # calc F matrix and matches in parallel\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            future1 = executor.submit(inf_models_ensemble, image_fpath_1=image_fpath_1, image_fpath_2=image_fpath_2) # 任务1: inf_models_ensemble, 获取匹配关键点\n",
    "            future2 = executor.submit(calc_F_matrix, item=prev_matches_item) # 任务2: calc_F_matrix, 计算上一对匹配关键点的F-matrix\n",
    "            future_list = [future1,future2]\n",
    "\n",
    "            finished, pending = futures.wait(\n",
    "                future_list, # 待完成的任务列表\n",
    "                return_when=futures.ALL_COMPLETED # 当所有任务完成时, 返回结果\n",
    "                )\n",
    "            \n",
    "            mkpts1_merge, mkpts2_merge = future1.result()\n",
    "            old_sample_id, F = future2.result()\n",
    "\n",
    "            # results are ready\n",
    "            F_dict[old_sample_id] = np.zeros((3, 3)) if F is None else F  # 存入F_dict\n",
    "            prev_matches_item = (sample_id, mkpts1_merge, mkpts2_merge) # update prev_matches_item\n",
    "    \n",
    "    try:\n",
    "        # clean up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# process last item\n",
    "if prev_matches_item is not None:\n",
    "    sample_id, F = calc_F_matrix(prev_matches_item) # 计算上一对匹配关键点的F-matrix\n",
    "    F_dict[sample_id] = np.zeros((3, 3)) if F is None else F  # 存入F_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FlattenMatrix(M, num_digits=8):\n",
    "    # Matrix to String\n",
    "    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n",
    "\n",
    "# 写入 submission.csv\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('sample_id,fundamental_matrix\\n')\n",
    "    for sample_id, F in F_dict.items():\n",
    "        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
